\begin{thebibliography}{50}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arjovsky et~al.(2016)Arjovsky, Shah, and Bengio]{arjovsky2016unitary}
Martin Arjovsky, Amar Shah, and Yoshua Bengio.
\newblock Unitary evolution recurrent neural networks.
\newblock In \emph{The International Conference on Machine Learning (ICML)},
  pages 1120--1128, 2016.

\bibitem[Baevski and Auli(2018)]{baevski2018adaptive}
Alexei Baevski and Michael Auli.
\newblock Adaptive input representations for neural language modeling.
\newblock \emph{arXiv preprint arXiv:1809.10853}, 2018.

\bibitem[Bai et~al.(2018)Bai, Kolter, and Koltun]{bai2018empirical}
Shaojie Bai, J~Zico Kolter, and Vladlen Koltun.
\newblock An empirical evaluation of generic convolutional and recurrent
  networks for sequence modeling.
\newblock \emph{arXiv preprint arXiv:1803.01271}, 2018.

\bibitem[Bai et~al.(2019)Bai, Kolter, and Koltun]{trellisnet}
Shaojie Bai, J~Zico Kolter, and Vladlen Koltun.
\newblock Trellis networks for sequence modeling.
\newblock In \emph{The International Conference on Learning Representations
  ({ICLR})}, 2019.

\bibitem[Chang et~al.(2017)Chang, Zhang, Han, Yu, Guo, Tan, Cui, Witbrock,
  Hasegawa-Johnson, and Huang]{chang2017dilated}
Shiyu Chang, Yang Zhang, Wei Han, Mo~Yu, Xiaoxiao Guo, Wei Tan, Xiaodong Cui,
  Michael Witbrock, Mark Hasegawa-Johnson, and Thomas~S Huang.
\newblock Dilated recurrent neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems
  ({NeurIPS})}, 2017.

\bibitem[Child et~al.(2019)Child, Gray, Radford, and
  Sutskever]{child2019generating}
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
\newblock Generating long sequences with sparse transformers.
\newblock \emph{arXiv preprint arXiv:1904.10509}, 2019.

\bibitem[Chilkuri and Eliasmith(2021)]{chilkuri2021parallelizing}
Narsimha Chilkuri and Chris Eliasmith.
\newblock Parallelizing legendre memory unit training.
\newblock \emph{The International Conference on Machine Learning (ICML)}, 2021.

\bibitem[Choromanski et~al.(2020)Choromanski, Likhosherstov, Dohan, Song, Gane,
  Sarlos, Hawkins, Davis, Mohiuddin, Kaiser, et~al.]{choromanski2020rethinking}
Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,
  Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin,
  Lukasz Kaiser, et~al.
\newblock Rethinking attention with performers.
\newblock In \emph{The International Conference on Learning Representations
  ({ICLR})}, 2020.

\bibitem[Dauphin et~al.(2017)Dauphin, Fan, Auli, and
  Grangier]{dauphin2017language}
Yann~N Dauphin, Angela Fan, Michael Auli, and David Grangier.
\newblock Language modeling with gated convolutional networks.
\newblock In \emph{International conference on machine learning}, pages
  933--941. PMLR, 2017.

\bibitem[De~Brouwer et~al.(2019)De~Brouwer, Simm, Arany, and Moreau]{de2019gru}
Edward De~Brouwer, Jaak Simm, Adam Arany, and Yves Moreau.
\newblock Gru-ode-bayes: Continuous modeling of sporadically-observed time
  series.
\newblock In \emph{Advances in Neural Information Processing Systems
  ({NeurIPS})}, 2019.

\bibitem[Donahue et~al.(2019)Donahue, McAuley, and
  Puckette]{Donahue2019AdversarialAS}
Chris Donahue, Julian McAuley, and Miller Puckette.
\newblock Adversarial audio synthesis.
\newblock In \emph{ICLR}, 2019.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Erichson et~al.(2021)Erichson, Azencot, Queiruga, Hodgkinson, and
  Mahoney]{erichson2021lipschitz}
N~Benjamin Erichson, Omri Azencot, Alejandro Queiruga, Liam Hodgkinson, and
  Michael~W Mahoney.
\newblock Lipschitz recurrent neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Goel et~al.(2022)Goel, Gu, Donahue, and R{\'e}]{goel2022sashimi}
Karan Goel, Albert Gu, Chris Donahue, and Christopher R{\'e}.
\newblock It's raw! audio generation with state-space models.
\newblock \emph{arXiv preprint arXiv:2202.09729}, 2022.

\bibitem[Golub and Van~Loan(2013)]{golub2013matrix}
Gene~H Golub and Charles~F Van~Loan.
\newblock \emph{Matrix computations}, volume~3.
\newblock JHU press, 2013.

\bibitem[Gu et~al.(2020{\natexlab{a}})Gu, Dao, Ermon, Rudra, and
  R{\'{e}}]{gu2020hippo}
Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R{\'{e}}.
\newblock Hippo: Recurrent memory with optimal polynomial projections.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020{\natexlab{a}}.

\bibitem[Gu et~al.(2020{\natexlab{b}})Gu, Gulcehre, Paine, Hoffman, and
  Pascanu]{gu2020improving}
Albert Gu, Caglar Gulcehre, Tom~Le Paine, Matt Hoffman, and Razvan Pascanu.
\newblock Improving the gating mechanism of recurrent neural networks.
\newblock In \emph{The International Conference on Machine Learning ({ICML})},
  2020{\natexlab{b}}.

\bibitem[Gu et~al.(2021)Gu, Johnson, Goel, Saab, Dao, Rudra, and
  R\'{e}]{gu2021lssl}
Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and
  Christopher R\'{e}.
\newblock Combining recurrent, convolutional, and continuous-time models with
  the structured learnable linear state space layer.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2021.

\bibitem[Gu et~al.(2022{\natexlab{a}})Gu, Gupta, Goel, and R\'e]{gu2022s4d}
Albert Gu, Ankit Gupta, Karan Goel, and Christopher R\'e.
\newblock On the parameterization and initialization of diagonal state space
  models.
\newblock \emph{arXiv preprint arXiv:2206.11893}, 2022{\natexlab{a}}.

\bibitem[Gu et~al.(2022{\natexlab{b}})Gu, Johnson, Timalsina, Rudra, and
  R\'e]{gu2022hippo}
Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher R\'e.
\newblock How to train your hippo: State space models with generalized basis
  projections.
\newblock \emph{arXiv preprint arXiv:2206.12037}, 2022{\natexlab{b}}.

\bibitem[Hochreiter and Schmidhuber(1997)]{lstm}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 9\penalty0 (8):\penalty0 1735--1780, 1997.

\bibitem[Katharopoulos et~al.(2020)Katharopoulos, Vyas, Pappas, and
  Fleuret]{katharopoulos2020transformers}
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran{\c{c}}ois
  Fleuret.
\newblock Transformers are rnns: Fast autoregressive transformers with linear
  attention.
\newblock In \emph{International Conference on Machine Learning}, pages
  5156--5165. PMLR, 2020.

\bibitem[Kidger et~al.(2020)Kidger, Morrill, Foster, and
  Lyons]{kidger2020neural}
Patrick Kidger, James Morrill, James Foster, and Terry Lyons.
\newblock Neural controlled differential equations for irregular time series.
\newblock \emph{arXiv preprint arXiv:2005.08926}, 2020.

\bibitem[Lezcano-Casado and Mart{\'\i}nez-Rubio(2019)]{lezcano2019cheap}
Mario Lezcano-Casado and David Mart{\'\i}nez-Rubio.
\newblock Cheap orthogonal constraints in neural networks: A simple
  parametrization of the orthogonal and unitary group.
\newblock In \emph{The International Conference on Machine Learning (ICML)},
  2019.

\bibitem[Li et~al.(2018)Li, Li, Cook, Zhu, and Gao]{indrnn}
Shuai Li, Wanqing Li, Chris Cook, Ce~Zhu, and Yanbo Gao.
\newblock Independently recurrent neural network ({IndRNN}): Building a longer
  and deeper {RNN}.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 5457--5466, 2018.

\bibitem[Lioutas and Guo(2020)]{lioutas2020time}
Vasileios Lioutas and Yuhong Guo.
\newblock Time-aware large kernel convolutions.
\newblock In \emph{International Conference on Machine Learning}, pages
  6172--6183. PMLR, 2020.

\bibitem[Merity et~al.(2018)Merity, Keskar, Bradbury, and
  Socher]{merity2018scalable}
Stephen Merity, Nitish~Shirish Keskar, James Bradbury, and Richard Socher.
\newblock Scalable language modeling: Wikitext-103 on a single gpu in 12 hours.
\newblock \emph{SysML}, 2018.

\bibitem[Oord et~al.(2016)Oord, Dieleman, Zen, Simonyan, Vinyals, Graves,
  Kalchbrenner, Senior, and Kavukcuoglu]{oord2016wavenet}
Aaron van~den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals,
  Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu.
\newblock Wavenet: A generative model for raw audio.
\newblock \emph{arXiv preprint arXiv:1609.03499}, 2016.

\bibitem[Pan(2001)]{pan2001structured}
Victor Pan.
\newblock \emph{Structured matrices and polynomials: unified superfast
  algorithms}.
\newblock Springer Science \& Business Media, 2001.

\bibitem[Pan(2017)]{pan2017fast}
Victor Pan.
\newblock Fast approximate computations with cauchy matrices and polynomials.
\newblock \emph{Mathematics of Computation}, 86\penalty0 (308):\penalty0
  2799--2826, 2017.

\bibitem[Pan(2015)]{pan2015transformations}
Victor~Y Pan.
\newblock Transformations of matrix structures work again.
\newblock \emph{Linear Algebra and Its Applications}, 465:\penalty0 107--138,
  2015.

\bibitem[Pascanu et~al.(2013)Pascanu, Mikolov, and
  Bengio]{pascanu2013difficulty}
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
\newblock On the difficulty of training recurrent neural networks.
\newblock In \emph{International conference on machine learning}, pages
  1310--1318, 2013.

\bibitem[Rae et~al.(2018)Rae, Dyer, Dayan, and Lillicrap]{rae2018fast}
Jack Rae, Chris Dyer, Peter Dayan, and Timothy Lillicrap.
\newblock Fast parametric learning with activation memorization.
\newblock \emph{The International Conference on Machine Learning (ICML)}, 2018.

\bibitem[Ramachandran et~al.(2017)Ramachandran, Paine, Khorrami, Babaeizadeh,
  Chang, Zhang, Hasegawa-Johnson, Campbell, and Huang]{ramachandran2017fast}
Prajit Ramachandran, Tom~Le Paine, Pooya Khorrami, Mohammad Babaeizadeh, Shiyu
  Chang, Yang Zhang, Mark~A Hasegawa-Johnson, Roy~H Campbell, and Thomas~S
  Huang.
\newblock Fast generation for convolutional autoregressive models.
\newblock \emph{arXiv preprint arXiv:1704.06001}, 2017.

\bibitem[Romero et~al.(2021)Romero, Kuzina, Bekkers, Tomczak, and
  Hoogendoorn]{romero2021ckconv}
David~W Romero, Anna Kuzina, Erik~J Bekkers, Jakub~M Tomczak, and Mark
  Hoogendoorn.
\newblock Ckconv: Continuous kernel convolution for sequential data.
\newblock \emph{arXiv preprint arXiv:2102.02611}, 2021.

\bibitem[Romero et~al.(2022)Romero, Bruintjes, Tomczak, Bekkers, Hoogendoorn,
  and van Gemert]{romero2022flexconv}
David~W Romero, Robert-Jan Bruintjes, Jakub~M Tomczak, Erik~J Bekkers, Mark
  Hoogendoorn, and Jan~C van Gemert.
\newblock Flexconv: Continuous kernel convolutions with differentiable kernel
  sizes.
\newblock In \emph{The International Conference on Learning Representations
  ({ICLR})}, 2022.

\bibitem[Rubanova et~al.(2019)Rubanova, Chen, and Duvenaud]{rubanova2019latent}
Yulia Rubanova, Tian~Qi Chen, and David~K Duvenaud.
\newblock Latent ordinary differential equations for irregularly-sampled time
  series.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5321--5331, 2019.

\bibitem[Rusch and Mishra(2021)]{rusch2021unicornn}
T~Konstantin Rusch and Siddhartha Mishra.
\newblock Unicornn: A recurrent model for learning very long time dependencies.
\newblock \emph{The International Conference on Machine Learning (ICML)}, 2021.

\bibitem[Salimans et~al.(2017)Salimans, Karpathy, Chen, and
  Kingma]{salimans2017pixelcnn++}
Tim Salimans, Andrej Karpathy, Xi~Chen, and Diederik~P Kingma.
\newblock Pixelcnn++: Improving the pixelcnn with discretized logistic mixture
  likelihood and other modifications.
\newblock \emph{arXiv preprint arXiv:1701.05517}, 2017.

\bibitem[Tay et~al.(2021)Tay, Dehghani, Abnar, Shen, Bahri, Pham, Rao, Yang,
  Ruder, and Metzler]{tay2021long}
Yi~Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham,
  Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler.
\newblock Long range arena : A benchmark for efficient transformers.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=qVyeW-grC2k}.

\bibitem[Tolstikhin et~al.(2021)Tolstikhin, Houlsby, Kolesnikov, Beyer, Zhai,
  Unterthiner, Yung, Keysers, Uszkoreit, Lucic, et~al.]{tolstikhin2021mlp}
Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai,
  Thomas Unterthiner, Jessica Yung, Daniel Keysers, Jakob Uszkoreit, Mario
  Lucic, et~al.
\newblock Mlp-mixer: An all-mlp architecture for vision.
\newblock \emph{arXiv preprint arXiv:2105.01601}, 2021.

\bibitem[Trinh et~al.(2018)Trinh, Dai, Luong, and Le]{trinh2018learning}
Trieu~H Trinh, Andrew~M Dai, Minh-Thang Luong, and Quoc~V Le.
\newblock Learning longer-term dependencies in {RNNs} with auxiliary losses.
\newblock In \emph{The International Conference on Machine Learning ({ICML})},
  2018.

\bibitem[Tustin(1947)]{tustin1947method}
Arnold Tustin.
\newblock A method of analysing the behaviour of linear systems in terms of
  time series.
\newblock \emph{Journal of the Institution of Electrical Engineers-Part IIA:
  Automatic Regulators and Servo Mechanisms}, 94\penalty0 (1):\penalty0
  130--142, 1947.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems
  ({NeurIPS})}, 2017.

\bibitem[Voelker et~al.(2019)Voelker, Kaji{\'c}, and
  Eliasmith]{voelker2019legendre}
Aaron Voelker, Ivana Kaji{\'c}, and Chris Eliasmith.
\newblock Legendre memory units: Continuous-time representation in recurrent
  neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  15544--15553, 2019.

\bibitem[Voelker(2019)]{voelker2019dynamical}
Aaron~Russell Voelker.
\newblock \emph{Dynamical systems in spiking neuromorphic hardware}.
\newblock PhD thesis, University of Waterloo, 2019.

\bibitem[Warden(2018)]{Warden2018SpeechCA}
Pete Warden.
\newblock Speech commands: A dataset for limited-vocabulary speech recognition.
\newblock \emph{ArXiv}, abs/1804.03209, 2018.

\bibitem[Woodbury(1950)]{woodbury1950}
Max~A Woodbury.
\newblock Inverting modified matrices.
\newblock \emph{Memorandum report}, 42:\penalty0 106, 1950.

\bibitem[Wu et~al.(2019)Wu, Fan, Baevski, Dauphin, and Auli]{wu2019pay}
Felix Wu, Angela Fan, Alexei Baevski, Yann~N Dauphin, and Michael Auli.
\newblock Pay less attention with lightweight and dynamic convolutions.
\newblock In \emph{The International Conference on Learning Representations
  ({ICLR})}, 2019.

\bibitem[Zhou et~al.(2021)Zhou, Zhang, Peng, Zhang, Li, Xiong, and
  Zhang]{haoyietal-informer-2021}
Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong,
  and Wancai Zhang.
\newblock Informer: Beyond efficient transformer for long sequence time-series
  forecasting.
\newblock In \emph{The Thirty-Fifth {AAAI} Conference on Artificial
  Intelligence, {AAAI} 2021, Virtual Conference}, volume~35, pages
  11106--11115. {AAAI} Press, 2021.

\end{thebibliography}
