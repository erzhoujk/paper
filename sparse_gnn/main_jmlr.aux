\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\bibstyle{abbrvnat}
\citation{harvey1990forecasting}
\citation{ortega2018graph,stankovic2020graph2,di2018adaptive,isufi2019forecasting}
\citation{bruna2014spectral,bronstein2017geometric,bacciu2020gentle,bronstein2021geometric}
\HyPL@Entry{0<</S/D>>}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{mei2016signal,variddhisai2020methods}
\citation{shang2021discrete,wu2020connecting}
\citation{satorras2022multivariate,rampavsek2022recipe}
\citation{vaswani2017attention}
\citation{scarselli2008graph}
\citation{bacciu2020gentle}
\citation{gilmer2017neural}
\citation{shang2021discrete,kipf2018neural}
\citation{wu2020connecting,deng2021graph}
\citation{zugner2021study}
\citation{shang2021discrete,kipf2018neural}
\citation{glasserman1991gradient,kingma2014auto}
\citation{paulus2020gradient}
\citation{rubinstein1969some,williams1992simple,mohamed2020monte}
\citation{mnih2014neural}
\newlabel{eq:loss-gradient-intro}{{1}{3}{}{equation.1.1}{}}
\citation{seo2018structured,li2018diffusion,yu2018spatio,wu2019graph,deng2021graph,cini2022filling,marisca2022learning,wu2021traversenet}
\citation{wu2019graph}
\citation{bai2020adaptive,oreshkin2020fcgaga}
\citation{satorras2022multivariate}
\citation{wu2020connecting}
\citation{deng2021graph}
\citation{zhang2022graph}
\citation{franceschi2019learning}
\citation{bengio2013estimating}
\citation{kipf2018neural}
\citation{shang2021discrete}
\citation{li2018diffusion}
\citation{maddison2017concrete,jang2017categorical}
\citation{kazi2022differentiable}
\citation{kool2019stochastic}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Works}{4}{section.2}\protected@file@percent }
\newlabel{s:related-works}{{2}{4}{}{section.2}{}}
\citation{niculae2023discrete}
\citation{jang2017categorical,maddison2017concrete,paulus2020gradient}
\citation{tucker2017rebar,grathwohl2018backpropagation}
\citation{mnih2014neural}
\citation{rennie2017self}
\citation{kool2020estimating}
\citation{correia2020efficient}
\citation{niepert2021implicit}
\citation{bengio2013estimating}
\citation{minervini2023adaptive}
\citation{mohamed2020monte}
\citation{niculae2023discrete}
\citation{schlichtkrull2018modeling}
\@writefile{toc}{\contentsline {section}{\numberline {3}Preliminaries}{5}{section.3}\protected@file@percent }
\newlabel{s:prelim}{{3}{5}{}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Spatiotemporal Time Series with Graph Side Information}{5}{subsection.3.1}\protected@file@percent }
\newlabel{s:prelim-graphs}{{3.1}{5}{}{subsection.3.1}{}}
\citation{gilmer2017neural}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Spatiotemporal Graph Neural Networks}{6}{subsection.3.2}\protected@file@percent }
\newlabel{s:stgn}{{3.2}{6}{}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Message-Passing Neural Networks}{6}{subsubsection.3.2.1}\protected@file@percent }
\newlabel{eq:mp}{{3}{6}{}{equation.3.3}{}}
\citation{gao2021equivalence}
\citation{kipf2018neural}
\citation{satorras2022multivariate}
\citation{seo2018structured,li2018diffusion}
\citation{frechet1948elements}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Spatiotemporal Architectures}{7}{subsubsection.3.2.2}\protected@file@percent }
\newlabel{eq:tts}{{7}{7}{}{equation.3.7}{}}
\newlabel{eq:stgnn-tts}{{10}{7}{}{equation.3.10}{}}
\newlabel{eq:pooling-temp}{{11}{7}{}{equation.3.11}{}}
\citation{jain2016statistical}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Mean Adjacency Matrices}{8}{subsection.3.3}\protected@file@percent }
\newlabel{s:prelim-frechet}{{3.3}{8}{}{subsection.3.3}{}}
\newlabel{eq:frechet-mean-l2}{{12}{8}{}{equation.3.12}{}}
\newlabel{eq:frechet-fun-l2}{{13}{8}{}{equation.3.13}{}}
\newlabel{eq:support-sns}{{14}{8}{}{equation.3.14}{}}
\newlabel{eq:frechet-fun-H}{{16}{8}{}{equation.3.16}{}}
\newlabel{eq:frechet-mean-H}{{17}{8}{}{equation.3.17}{}}
\citation{velivckovic2018graph}
\@writefile{toc}{\contentsline {section}{\numberline {4}Problem Formulation}{9}{section.4}\protected@file@percent }
\newlabel{s:problem}{{4}{9}{}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Graph Learning from Spatiotemporal Time Series}{9}{subsection.4.1}\protected@file@percent }
\newlabel{eq:generic-model}{{18}{9}{}{equation.4.18}{}}
\newlabel{eq:generic-losses}{{19}{9}{}{equation.4.19}{}}
\citation{paske2019pytorch,abadi2015tensorflow}
\citation{schulman2015gradient,weber2019credit,mohamed2020monte}
\citation{foerster2018dice,bingham2019pyro,krieken2021storchastic,dillon2017tensorflow}
\citation{mohamed2020monte}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Core Challenge}{10}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Score-based Sparse Graph Learning from Spatiotemporal Time Series}{10}{section.5}\protected@file@percent }
\newlabel{s:lsd}{{5}{10}{}{section.5}{}}
\citation{l1995note,mohamed2020monte}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Overview of the learning architecture. The graph learning module samples a graph used to propagate information along the spatial dimension in $F_\psi $; predictions and samples are used to compute costs and log-likelihoods. Gradient estimates are propagated back to the respective modules.\relax }}{11}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:framework}{{1}{11}{Overview of the learning architecture. The graph learning module samples a graph used to propagate information along the spatial dimension in $F_\psi $; predictions and samples are used to compute costs and log-likelihoods. Gradient estimates are propagated back to the respective modules.\relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Estimating Gradients for Stochastic Message-Passing Networks}{11}{subsection.5.1}\protected@file@percent }
\newlabel{s:smp}{{5.1}{11}{}{subsection.5.1}{}}
\newlabel{eq:interchange}{{21}{11}{}{equation.5.21}{}}
\newlabel{eq:score-function-identity}{{22}{11}{}{equation.5.22}{}}
\newlabel{eq:score-function-identity-graph}{{23}{11}{}{equation.5.23}{}}
\citation{shang2021discrete,kipf2018neural}
\citation{franceschi2019learning,shang2021discrete}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Graph Distributions, Graphs Sampling, and Graphs Likelihood}{12}{subsection.5.2}\protected@file@percent }
\newlabel{s:sampling}{{5.2}{12}{}{subsection.5.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}Binary Edge Sampler}{12}{subsubsection.5.2.1}\protected@file@percent }
\citation{kool2019stochastic}
\citation{kool2019stochastic}
\citation{huijben2022review,kool2020estimating}
\citation{kool2019stochastic,kool2020estimating}
\citation{kool2020estimating}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.2}Subset Neighborhood Sampler}{13}{subsubsection.5.2.2}\protected@file@percent }
\newlabel{s:sns}{{5.2.2}{13}{}{subsubsection.5.2.2}{}}
\newlabel{e:unord}{{25}{13}{}{equation.5.25}{}}
\newlabel{e:logsumexp}{{26}{13}{}{equation.5.26}{}}
\newlabel{e:gmax}{{27}{13}{}{equation.5.27}{}}
\citation{kipf2016variational,kipf2018neural}
\newlabel{eq:sns-score-rewriting}{{31}{14}{}{equation.5.31}{}}
\newlabel{eq:sns-apporx-score}{{32}{14}{}{equation.5.32}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Parametrizing}{14}{subsection.5.3}\protected@file@percent }
\newlabel{s:ptheta}{{5.3}{14}{}{subsection.5.3}{}}
\newlabel{e:phi}{{33}{14}{}{equation.5.33}{}}
\citation{mohamed2020monte}
\citation{sutton1999policy,mnih2016asynchronous}
\@writefile{toc}{\contentsline {section}{\numberline {6}Reducing the Variance of the Estimator}{15}{section.6}\protected@file@percent }
\newlabel{s:variance}{{6}{15}{}{section.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Control Variates and Baselines}{15}{subsection.6.1}\protected@file@percent }
\newlabel{eq:score-function-identity-baseline}{{34}{15}{}{equation.6.34}{}}
\newlabel{eq:beta-star}{{35}{15}{}{equation.6.35}{}}
\newlabel{eq:beta-hat}{{37}{16}{}{equation.6.37}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Baseline for BES}{16}{subsection.6.2}\protected@file@percent }
\newlabel{sec:frechet-mean-bes}{{6.2}{16}{}{subsection.6.2}{}}
\newlabel{prop:mu-frechet-bes}{{1}{16}{}{theorem.1}{}}
\newlabel{lemma:frechet-fun}{{2}{16}{}{theorem.2}{}}
\newlabel{eq:frechet-minimum}{{39}{16}{}{equation.6.39}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Baseline for SNS}{17}{subsection.6.3}\protected@file@percent }
\newlabel{sec:frechet-mean-sns}{{6.3}{17}{}{subsection.6.3}{}}
\newlabel{eq:SNS-baseline}{{40}{17}{}{equation.6.40}{}}
\newlabel{prop:mu-frechet-sns}{{3}{17}{}{theorem.3}{}}
\newlabel{eq:directed-knn-set}{{42}{17}{}{equation.6.42}{}}
\newlabel{eq:SNS-fr-obj}{{44}{17}{}{equation.6.44}{}}
\newlabel{eq:frfun-sns-lin}{{47}{17}{}{equation.6.47}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Overview of the learning architecture with layer-wise sampling and surrogate objective. The graph module samples a graph for each MP layer of predictor $F_\psi $.\relax }}{18}{figure.caption.2}\protected@file@percent }
\newlabel{fig:framework-extended}{{2}{18}{Overview of the learning architecture with layer-wise sampling and surrogate objective. The graph module samples a graph for each MP layer of predictor $F_\psi $.\relax }{figure.caption.2}{}}
\newlabel{lemma:pi-geq-pj}{{4}{18}{}{theorem.4}{}}
\newlabel{eq:sns-linear-program}{{49}{18}{}{equation.6.49}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Layer-wise Sampling and Surrogate Objective}{18}{section.7}\protected@file@percent }
\newlabel{s:surrogate}{{7}{18}{}{section.7}{}}
\newlabel{p:grad}{{5}{18}{}{theorem.5}{}}
\newlabel{eq:loss-decomposition}{{50}{19}{}{equation.7.50}{}}
\citation{isufi2019forecasting}
\citation{zambon2022az}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Surrogate Objective}{20}{subsection.7.1}\protected@file@percent }
\newlabel{e:approx2}{{57}{20}{}{equation.7.57}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Experiments}{20}{section.8}\protected@file@percent }
\newlabel{sec:exp}{{8}{20}{}{section.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Datasets}{20}{subsection.8.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Additional information on the considered datasets.\relax }}{20}{table.caption.3}\protected@file@percent }
\newlabel{t:datasets}{{1}{20}{Additional information on the considered datasets.\relax }{table.caption.3}{}}
\citation{zambon2022az}
\citation{yi2016st,cini2022filling,marisca2022learning}
\citation{yi2016st}
\citation{cini2022filling}
\citation{jagadish2014big,li2018diffusion}
\citation{wu2019graph}
\citation{wu2019graph}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Controlled Environment Experiments}{21}{subsection.8.2}\protected@file@percent }
\newlabel{sec:gpvar}{{8.2}{21}{}{subsection.8.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.1}Graph Identification and Time Series Forecasting}{21}{subsubsection.8.2.1}\protected@file@percent }
\newlabel{sec:gpvar-exp}{{8.2.1}{21}{}{subsubsection.8.2.1}{}}
\newlabel{fig:gpvar-a}{{3a}{22}{}{figure.caption.4}{}}
\newlabel{sub@fig:gpvar-a}{{a}{22}{}{figure.caption.4}{}}
\newlabel{fig:gpvar-b}{{3b}{22}{}{figure.caption.4}{}}
\newlabel{sub@fig:gpvar-b}{{b}{22}{}{figure.caption.4}{}}
\newlabel{fig:gpvar-c}{{3c}{22}{}{figure.caption.4}{}}
\newlabel{sub@fig:gpvar-c}{{c}{22}{}{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Experiments on GPVAR. All the curves show the validation MAE after each training epoch. \relax }}{22}{figure.caption.4}\protected@file@percent }
\newlabel{fig:gpvar}{{3}{22}{Experiments on GPVAR. All the curves show the validation MAE after each training epoch. \relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Sensitivity analysis on $\lambda $ for the surrogate objective.\relax }}{23}{figure.caption.5}\protected@file@percent }
\newlabel{fig:lam}{{4}{23}{Sensitivity analysis on $\lambda $ for the surrogate objective.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Sensitivity analysis on $K$ for SNS.\relax }}{23}{figure.caption.6}\protected@file@percent }
\newlabel{fig:ks}{{5}{23}{Sensitivity analysis on $K$ for SNS.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.2}Sensitivity Analysis}{23}{subsubsection.8.2.2}\protected@file@percent }
\citation{maddison2017concrete}
\citation{bengio2013estimating}
\newlabel{fig:gpvar-comp-est-a}{{6a}{24}{}{figure.caption.7}{}}
\newlabel{sub@fig:gpvar-comp-est-a}{{a}{24}{}{figure.caption.7}{}}
\newlabel{fig:gpvar-comp-est-b}{{6b}{24}{}{figure.caption.7}{}}
\newlabel{sub@fig:gpvar-comp-est-b}{{b}{24}{}{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Comparison of different estimators on the joint training settings in GPVAR.\relax }}{24}{figure.caption.7}\protected@file@percent }
\newlabel{fig:gpvar-comp-est}{{6}{24}{Comparison of different estimators on the joint training settings in GPVAR.\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.3}Comparison with Path-wise and Straight-through Estimators}{24}{subsubsection.8.2.3}\protected@file@percent }
\citation{chung2014empirical}
\citation{satorras2022multivariate}
\citation{satorras2022multivariate}
\citation{wu2020connecting}
\citation{zugner2021study}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}Real-World Datasets}{25}{subsection.8.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.3.1}Graph Identification in AQI}{25}{subsubsection.8.3.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces AQI experiment.\relax }}{25}{table.caption.8}\protected@file@percent }
\newlabel{t:aqi}{{2}{25}{AQI experiment.\relax }{table.caption.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.3.2}Joint Training and Forecasting in Traffic Datasets}{25}{subsubsection.8.3.2}\protected@file@percent }
\citation{cini2022filling}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Results on the traffic datasets.\relax }}{26}{table.caption.9}\protected@file@percent }
\newlabel{t:traffic}{{3}{26}{Results on the traffic datasets.\relax }{table.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Computational scalability of the proposed estimator against the straight-through method.\relax }}{26}{figure.caption.10}\protected@file@percent }
\newlabel{fig:scalability}{{7}{26}{Computational scalability of the proposed estimator against the straight-through method.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4}Scalability}{26}{subsection.8.4}\protected@file@percent }
\newlabel{sec:scalability}{{8.4}{26}{}{subsection.8.4}{}}
\citation{kool2020estimating}
\citation{niepert2021implicit,minervini2023adaptive}
\@writefile{toc}{\contentsline {section}{\numberline {9}Conclusions}{27}{section.9}\protected@file@percent }
\newlabel{s:conclusion}{{9}{27}{}{section.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Deferred Proofs}{28}{appendix.A}\protected@file@percent }
\newlabel{a:proofs}{{A}{28}{Appendix}{appendix.A}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Proof of Lemma\nobreakspace  {}\ref {lemma:frechet-fun}}{28}{subsection.A.1}\protected@file@percent }
\newlabel{a:proof-lemma:frechet-fun}{{A.1}{28}{Appendix}{subsection.A.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Proof of Lemma\nobreakspace  {}\ref {lemma:pi-geq-pj}}{28}{subsection.A.2}\protected@file@percent }
\newlabel{eq:phii-ge-phij}{{69}{28}{Appendix}{equation.A.69}{}}
\newlabel{eq:int-mu-ni}{{71}{28}{Appendix}{equation.A.71}{}}
\citation{kool2020estimating}
\citation{paske2019pytorch}
\citation{fey2019fast}
\citation{Cini_Torch_Spatiotemporal_2022}
\citation{Falcon_PyTorch_Lightning_2019}
\citation{harris2020array}
\@writefile{toc}{\contentsline {section}{\numberline {B}Details on the Computation of the SNS Likelihood}{29}{appendix.B}\protected@file@percent }
\newlabel{a:trapezoid}{{B}{29}{Appendix}{appendix.B}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Experiments Details}{29}{appendix.C}\protected@file@percent }
\newlabel{a:exp}{{C}{29}{Appendix}{appendix.C}{}}
\citation{neptune2021neptune}
\citation{ramachandran2017searching}
\citation{satorras2022multivariate}
\citation{satorras2022multivariate}
\citation{satorras2022multivariate}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.1}Synthetic Experiments}{30}{subsection.C.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C.2}AQI Experiment}{30}{subsection.C.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C.3}Traffic Experiment}{30}{subsection.C.3}\protected@file@percent }
\bibdata{main.bib}
\bibcite{abadi2015tensorflow}{{1}{2015}{{Abadi et~al.}}{{Abadi, Agarwal, Barham, Brevdo, Chen, Citro, Corrado, Davis, Dean, Devin, Ghemawat, Goodfellow, Harp, Irving, Isard, Jia, Jozefowicz, Kaiser, Kudlur, Levenberg, Man\'{e}, Monga, Moore, Murray, Olah, Schuster, Shlens, Steiner, Sutskever, Talwar, Tucker, Vanhoucke, Vasudevan, Vi\'{e}gas, Vinyals, Warden, Wattenberg, Wicke, Yu, and Zheng}}}
\bibcite{bacciu2020gentle}{{2}{2020}{{Bacciu et~al.}}{{Bacciu, Errica, Micheli, and Podda}}}
\bibcite{bai2020adaptive}{{3}{2020}{{Bai et~al.}}{{Bai, Yao, Li, Wang, and Wang}}}
\bibcite{bengio2013estimating}{{4}{2013}{{Bengio et~al.}}{{Bengio, L{\'e}onard, and Courville}}}
\bibcite{bingham2019pyro}{{5}{2019}{{Bingham et~al.}}{{Bingham, Chen, Jankowiak, Obermeyer, Pradhan, Karaletsos, Singh, Szerlip, Horsfall, and Goodman}}}
\bibcite{bronstein2017geometric}{{6}{2017}{{Bronstein et~al.}}{{Bronstein, Bruna, LeCun, Szlam, and Vandergheynst}}}
\bibcite{bronstein2021geometric}{{7}{2021}{{Bronstein et~al.}}{{Bronstein, Bruna, Cohen, and Veli{\v {c}}kovi{\'c}}}}
\bibcite{bruna2014spectral}{{8}{2014}{{Bruna et~al.}}{{Bruna, Zaremba, Szlam, and LeCun}}}
\bibcite{chung2014empirical}{{9}{2014}{{Chung et~al.}}{{Chung, Gulcehre, Cho, and Bengio}}}
\bibcite{Cini_Torch_Spatiotemporal_2022}{{10}{2022}{{Cini and Marisca}}{{}}}
\bibcite{cini2022filling}{{11}{2022}{{Cini et~al.}}{{Cini, Marisca, and Alippi}}}
\bibcite{correia2020efficient}{{12}{2020}{{Correia et~al.}}{{Correia, Niculae, Aziz, and Martins}}}
\bibcite{deng2021graph}{{13}{2021}{{Deng and Hooi}}{{}}}
\bibcite{di2018adaptive}{{14}{2018}{{Di~Lorenzo et~al.}}{{Di~Lorenzo, Banelli, Isufi, Barbarossa, and Leus}}}
\bibcite{dillon2017tensorflow}{{15}{2017}{{Dillon et~al.}}{{Dillon, Langmore, Tran, Brevdo, Vasudevan, Moore, Patton, Alemi, Hoffman, and Saurous}}}
\bibcite{Falcon_PyTorch_Lightning_2019}{{16}{2019}{{Falcon and {The PyTorch Lightning team}}}{{}}}
\bibcite{fey2019fast}{{17}{2019}{{Fey and Lenssen}}{{}}}
\bibcite{foerster2018dice}{{18}{2018}{{Foerster et~al.}}{{Foerster, Farquhar, Al-Shedivat, Rockt{\"a}schel, Xing, and Whiteson}}}
\bibcite{franceschi2019learning}{{19}{2019}{{Franceschi et~al.}}{{Franceschi, Niepert, Pontil, and He}}}
\bibcite{frechet1948elements}{{20}{1948}{{Fr{\'e}chet}}{{}}}
\bibcite{gao2021equivalence}{{21}{2022}{{Gao and Ribeiro}}{{}}}
\bibcite{gilmer2017neural}{{22}{2017}{{Gilmer et~al.}}{{Gilmer, Schoenholz, Riley, Vinyals, and Dahl}}}
\bibcite{glasserman1991gradient}{{23}{1991}{{Glasserman and Ho}}{{}}}
\bibcite{grathwohl2018backpropagation}{{24}{2018}{{Grathwohl et~al.}}{{Grathwohl, Choi, Wu, Roeder, and Duvenaud}}}
\bibcite{harris2020array}{{25}{2020}{{Harris et~al.}}{{Harris, Millman, Van Der~Walt, Gommers, Virtanen, Cournapeau, Wieser, Taylor, Berg, Smith, et~al.}}}
\bibcite{harvey1990forecasting}{{26}{1990}{{Harvey et~al.}}{{}}}
\bibcite{huijben2022review}{{27}{2022}{{Huijben et~al.}}{{Huijben, Kool, Paulus, and Van~Sloun}}}
\bibcite{isufi2019forecasting}{{28}{2019}{{Isufi et~al.}}{{Isufi, Loukas, Perraudin, and Leus}}}
\bibcite{jagadish2014big}{{29}{2014}{{Jagadish et~al.}}{{Jagadish, Gehrke, Labrinidis, Papakonstantinou, Patel, Ramakrishnan, and Shahabi}}}
\bibcite{jain2016statistical}{{30}{2016}{{Jain}}{{}}}
\bibcite{jang2017categorical}{{31}{2017}{{Jang et~al.}}{{Jang, Gu, and Poole}}}
\bibcite{kazi2022differentiable}{{32}{2022}{{Kazi et~al.}}{{Kazi, Cosmo, Ahmadi, Navab, and Bronstein}}}
\bibcite{kingma2014auto}{{33}{2013}{{Kingma and Welling}}{{}}}
\bibcite{kipf2018neural}{{34}{2018}{{Kipf et~al.}}{{Kipf, Fetaya, Wang, Welling, and Zemel}}}
\bibcite{kipf2016variational}{{35}{2016}{{Kipf and Welling}}{{}}}
\bibcite{kool2019stochastic}{{36}{2019}{{Kool et~al.}}{{Kool, Van~Hoof, and Welling}}}
\bibcite{kool2020estimating}{{37}{2020}{{Kool et~al.}}{{Kool, van Hoof, and Welling}}}
\bibcite{l1995note}{{38}{1995}{{L'Ecuyer}}{{}}}
\bibcite{li2018diffusion}{{39}{2018}{{Li et~al.}}{{Li, Yu, Shahabi, and Liu}}}
\bibcite{maddison2017concrete}{{40}{2017}{{Maddison et~al.}}{{Maddison, Mnih, and Teh}}}
\bibcite{marisca2022learning}{{41}{2022}{{Marisca et~al.}}{{Marisca, Cini, and Alippi}}}
\bibcite{mei2016signal}{{42}{2016}{{Mei and Moura}}{{}}}
\bibcite{minervini2023adaptive}{{43}{2023}{{Minervini et~al.}}{{Minervini, Franceschi, and Niepert}}}
\bibcite{mnih2014neural}{{44}{2014}{{Mnih and Gregor}}{{}}}
\bibcite{mnih2016asynchronous}{{45}{2016}{{Mnih et~al.}}{{Mnih, Badia, Mirza, Graves, Lillicrap, Harley, Silver, and Kavukcuoglu}}}
\bibcite{mohamed2020monte}{{46}{2020}{{Mohamed et~al.}}{{Mohamed, Rosca, Figurnov, and Mnih}}}
\bibcite{neptune2021neptune}{{47}{2021}{{neptune.ai}}{{}}}
\bibcite{niculae2023discrete}{{48}{2023}{{Niculae et~al.}}{{Niculae, Corro, Nangia, Mihaylova, and Martins}}}
\bibcite{niepert2021implicit}{{49}{2021}{{Niepert et~al.}}{{Niepert, Minervini, and Franceschi}}}
\bibcite{oreshkin2020fcgaga}{{50}{2021}{{Oreshkin et~al.}}{{Oreshkin, Amini, Coyle, and Coates}}}
\bibcite{ortega2018graph}{{51}{2018}{{Ortega et~al.}}{{Ortega, Frossard, Kova{\v {c}}evi{\'c}, Moura, and Vandergheynst}}}
\bibcite{paske2019pytorch}{{52}{2019}{{Paszke et~al.}}{{Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison, Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala}}}
\bibcite{paulus2020gradient}{{53}{2020}{{Paulus et~al.}}{{Paulus, Choi, Tarlow, Krause, and Maddison}}}
\bibcite{ramachandran2017searching}{{54}{2017}{{Ramachandran et~al.}}{{Ramachandran, Zoph, and Le}}}
\bibcite{rampavsek2022recipe}{{55}{2022}{{Ramp{\'a}{\v {s}}ek et~al.}}{{Ramp{\'a}{\v {s}}ek, Galkin, Dwivedi, Luu, Wolf, and Beaini}}}
\bibcite{rennie2017self}{{56}{2017}{{Rennie et~al.}}{{Rennie, Marcheret, Mroueh, Ross, and Goel}}}
\bibcite{rubinstein1969some}{{57}{1969}{{Rubinstein}}{{}}}
\bibcite{satorras2022multivariate}{{58}{2022}{{Satorras et~al.}}{{Satorras, Rangapuram, and Januschowski}}}
\bibcite{scarselli2008graph}{{59}{2008}{{Scarselli et~al.}}{{Scarselli, Gori, Tsoi, Hagenbuchner, and Monfardini}}}
\bibcite{schlichtkrull2018modeling}{{60}{2018}{{Schlichtkrull et~al.}}{{Schlichtkrull, Kipf, Bloem, Berg, Titov, and Welling}}}
\bibcite{schulman2015gradient}{{61}{2015}{{Schulman et~al.}}{{Schulman, Heess, Weber, and Abbeel}}}
\bibcite{seo2018structured}{{62}{2018}{{Seo et~al.}}{{Seo, Defferrard, Vandergheynst, and Bresson}}}
\bibcite{shang2021discrete}{{63}{2021}{{Shang and Chen}}{{}}}
\bibcite{stankovic2020graph2}{{64}{2020}{{Stankovi{\'c} et~al.}}{{Stankovi{\'c}, Mandic, Dakovi{\'c}, Brajovi{\'c}, Scalzo, Li, Constantinides, et~al.}}}
\bibcite{sutton1999policy}{{65}{1999}{{Sutton et~al.}}{{Sutton, McAllester, Singh, and Mansour}}}
\bibcite{tucker2017rebar}{{66}{2017}{{Tucker et~al.}}{{Tucker, Mnih, Maddison, Lawson, and Sohl-Dickstein}}}
\bibcite{krieken2021storchastic}{{67}{2021}{{van Krieken et~al.}}{{van Krieken, Tomczak, and Teije}}}
\bibcite{variddhisai2020methods}{{68}{2020}{{Variddhisai and Mandic}}{{}}}
\bibcite{vaswani2017attention}{{69}{2017}{{Vaswani et~al.}}{{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin}}}
\bibcite{velivckovic2018graph}{{70}{2018}{{Veli{\v {c}}kovi{\'c} et~al.}}{{Veli{\v {c}}kovi{\'c}, Cucurull, Casanova, Romero, Li{\`o}, and Bengio}}}
\bibcite{weber2019credit}{{71}{2019}{{Weber et~al.}}{{Weber, Heess, Buesing, and Silver}}}
\bibcite{williams1992simple}{{72}{1992}{{Williams}}{{}}}
\bibcite{wu2019graph}{{73}{2019}{{Wu et~al.}}{{Wu, Pan, Long, Jiang, and Zhang}}}
\bibcite{wu2020connecting}{{74}{2020}{{Wu et~al.}}{{Wu, Pan, Long, Jiang, Chang, and Zhang}}}
\bibcite{wu2021traversenet}{{75}{2022}{{Wu et~al.}}{{Wu, Zheng, Pan, Gan, Long, and Karypis}}}
\bibcite{yi2016st}{{76}{2016}{{Yi et~al.}}{{Yi, Zheng, Zhang, and Li}}}
\bibcite{yu2018spatio}{{77}{2018}{{Yu et~al.}}{{Yu, Yin, and Zhu}}}
\bibcite{zambon2022az}{{78}{2022}{{Zambon and Alippi}}{{}}}
\bibcite{zhang2022graph}{{79}{2022}{{Zhang et~al.}}{{Zhang, Zeman, Tsiligkaridis, and Zitnik}}}
\bibcite{zugner2021study}{{80}{2021}{{Z{\"u}gner et~al.}}{{Z{\"u}gner, Aubet, Satorras, Januschowski, G{\"u}nnemann, and Gasthaus}}}
\newlabel{LastPage}{{}{36}{}{page.36}{}}
\xdef\lastpage@lastpage{36}
\xdef\lastpage@lastpageHy{36}
\gdef \@abspage@last{36}
